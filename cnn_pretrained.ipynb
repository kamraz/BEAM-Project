{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torchvision.io import ImageReadMode\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize model with the best available weights\n",
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.imgs = list(sorted(os.listdir(root)))\n",
    "        # remove files without image extensions\n",
    "        self.imgs = [img for img in self.imgs if img.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, self.imgs[idx])\n",
    "        # read the image and remove alpha channel\n",
    "        img = read_image(img_path, ImageReadMode.RGB)\n",
    "        return img\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "test_data = TestDataset(\"Instructions\", transforms=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Duke Farms_2022_03_08_13_58_46.png', 'Duke Farms_2022_03_08_14_57_27.png', 'Duke Farms_2022_03_14_23_06_55.png', 'Duke Farms_2022_03_16_17_01_15.png', 'Duke Farms_2022_04_02_09_26_42.png', 'Duke Farms_2022_04_26_07_53_24.png', 'Duke Farms_2022_05_07_18_35_13.png', 'National Arboretum A_2022_03_21_19_05_20.png', 'National Arboretum A_2022_03_29_19_43_05.png', 'National Arboretum A_2022_04_04_06_28_34.png', 'National Arboretum A_2022_04_04_08_37_52.png', 'National Arboretum A_2022_04_28_11_13_18.png', 'National Arboretum A_2022_05_02_01_24_18.png', 'National Arboretum A_2022_05_07_03_23_45.png', 'National Arboretum A_2022_05_07_10_34_48.png', 'National Arboretum A_2022_05_09_03_41_02.png', 'National Arboretum A_2022_05_15_08_33_14.png']\n"
     ]
    }
   ],
   "source": [
    "print(test_data.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boxes': tensor([[461.5152, 267.2453, 871.1411, 710.0000]], device='cuda:0'), 'labels': tensor([16], device='cuda:0'), 'scores': tensor([0.9450], device='cuda:0')}\n",
      "{'boxes': tensor([[318.1213, 101.3299, 718.3945, 530.3976]], device='cuda:0'), 'labels': tensor([16], device='cuda:0'), 'scores': tensor([0.9782], device='cuda:0')}\n",
      "{'boxes': tensor([[334.8059, 192.9951, 604.6642, 499.3461]], device='cuda:0'), 'labels': tensor([16], device='cuda:0'), 'scores': tensor([0.9670], device='cuda:0')}\n",
      "{'boxes': tensor([], device='cuda:0', size=(0, 4)), 'labels': tensor([], device='cuda:0', dtype=torch.int64), 'scores': tensor([], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/eagles/lib/python3.9/site-packages/torchvision/utils.py:215: UserWarning: boxes doesn't contain any box. No box was drawn\n",
      "  warnings.warn(\"boxes doesn't contain any box. No box was drawn\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boxes': tensor([], device='cuda:0', size=(0, 4)), 'labels': tensor([], device='cuda:0', dtype=torch.int64), 'scores': tensor([], device='cuda:0')}\n",
      "{'boxes': tensor([], device='cuda:0', size=(0, 4)), 'labels': tensor([], device='cuda:0', dtype=torch.int64), 'scores': tensor([], device='cuda:0')}\n",
      "{'boxes': tensor([[447.4741, 249.1185, 564.0780, 364.1725]], device='cuda:0'), 'labels': tensor([16], device='cuda:0'), 'scores': tensor([0.9695], device='cuda:0')}\n",
      "{'boxes': tensor([[615.9965, 240.3985, 861.4205, 531.9827]], device='cuda:0'), 'labels': tensor([16], device='cuda:0'), 'scores': tensor([0.9756], device='cuda:0')}\n",
      "{'boxes': tensor([], device='cuda:0', size=(0, 4)), 'labels': tensor([], device='cuda:0', dtype=torch.int64), 'scores': tensor([], device='cuda:0')}\n",
      "{'boxes': tensor([[516.9413, 122.7436, 898.1485, 545.5803]], device='cuda:0'), 'labels': tensor([16], device='cuda:0'), 'scores': tensor([0.9848], device='cuda:0')}\n",
      "{'boxes': tensor([[173.4897,  83.8651, 714.5428, 751.8914]], device='cuda:0'), 'labels': tensor([16], device='cuda:0'), 'scores': tensor([0.9976], device='cuda:0')}\n",
      "{'boxes': tensor([[448.7550, 344.2188, 682.0931, 549.5637],\n",
      "        [733.1168, 311.8940, 926.1931, 533.0712]], device='cuda:0'), 'labels': tensor([16, 23], device='cuda:0'), 'scores': tensor([0.9147, 0.9045], device='cuda:0')}\n",
      "{'boxes': tensor([[726.3424,   1.0474, 932.2842, 161.1727]], device='cuda:0'), 'labels': tensor([16], device='cuda:0'), 'scores': tensor([0.9862], device='cuda:0')}\n",
      "{'boxes': tensor([], device='cuda:0', size=(0, 4)), 'labels': tensor([], device='cuda:0', dtype=torch.int64), 'scores': tensor([], device='cuda:0')}\n",
      "{'boxes': tensor([[ 800.5475,  353.0040, 1086.7544,  669.2847]], device='cuda:0'), 'labels': tensor([16], device='cuda:0'), 'scores': tensor([0.9970], device='cuda:0')}\n",
      "{'boxes': tensor([], device='cuda:0', size=(0, 4)), 'labels': tensor([], device='cuda:0', dtype=torch.int64), 'scores': tensor([], device='cuda:0')}\n",
      "{'boxes': tensor([[775.6340,  49.2937, 924.4930, 343.5407]], device='cuda:0'), 'labels': tensor([16], device='cuda:0'), 'scores': tensor([0.9991], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model = model.to('cuda')\n",
    "    for _, batch in enumerate(test_data_loader):\n",
    "        prediction = model(preprocess(batch.to('cuda')))[0]\n",
    "        print(prediction)\n",
    "        for image in batch:\n",
    "            labels = []\n",
    "            for i in range(len(prediction[\"labels\"])):\n",
    "                labels.append(weights.meta[\"categories\"][prediction[\"labels\"][i]] + \" \" + str(round(prediction[\"scores\"][i].item(), 3)))\n",
    "\n",
    "            box = draw_bounding_boxes(image, boxes=prediction[\"boxes\"],\n",
    "                                    labels=labels,\n",
    "                                    colors=\"red\",\n",
    "                                    width=4)\n",
    "            im = to_pil_image(box.detach())\n",
    "            im.save(\"test.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('eagles')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0925fae0ace981ccb72920df887c03553dc37f40775dd20c5d03ad1279f2cb4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
