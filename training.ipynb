{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import SmallDataset\n",
    "from models import get_model\n",
    "from utils.transforms import get_transform\n",
    "import utils.engine as engine\n",
    "\n",
    "import math\n",
    "import sys\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(num_classes=5)\n",
    "dataset = SmallDataset(\"/home/kamranzolfonoon/eagle-test-bucket\", \"/home/kamranzolfonoon/eagle-test-bucket/image_labels.json\", get_transform(train=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4,\n",
      "        3, 4, 3, 4, 4, 3, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4,\n",
      "        4, 4, 4, 3, 4, 3, 4, 4, 4, 4, 4, 4, 3, 4, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4,\n",
      "        3, 3, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "# images,targets = next(iter(data_loader))\n",
    "# images = list(image for image in images)\n",
    "# targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "# output = model(images,targets)\n",
    "# model.eval()\n",
    "# x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "# prediction = model(x)\n",
    "# print(prediction[0][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/eagles/lib/python3.9/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0, LOSS: 0.8220156936298514\n",
      "Epoch:  1\n",
      "EPOCH: 1, LOSS: 0.7844789110718758\n",
      "Epoch:  2\n",
      "EPOCH: 2, LOSS: 0.6972861741909958\n",
      "Epoch:  3\n",
      "EPOCH: 3, LOSS: 0.584196624341705\n",
      "Epoch:  4\n",
      "EPOCH: 4, LOSS: 0.49866288011902377\n",
      "Epoch:  5\n",
      "EPOCH: 5, LOSS: 0.42476456454941924\n",
      "Epoch:  6\n",
      "EPOCH: 6, LOSS: 0.3777401862150067\n",
      "Epoch:  7\n",
      "EPOCH: 7, LOSS: 0.34578504969536417\n",
      "Epoch:  8\n",
      "EPOCH: 8, LOSS: 0.3210454237573024\n",
      "Epoch:  9\n",
      "EPOCH: 9, LOSS: 0.2932579883947059\n",
      "That's it!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # split the dataset in train and test set\n",
    "# indices = torch.randperm(len(dataset)).tolist()\n",
    "# dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=2,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=3,\n",
    "                                                gamma=0.1)\n",
    "\n",
    "# let's train it for 10 epochs\n",
    "num_epochs = 10\n",
    "loss_over_run = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_over_epoch = []\n",
    "    model.train()\n",
    "\n",
    "    print(\"Epoch: \", epoch)\n",
    "\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1.0 / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
    "        )\n",
    "\n",
    "    for images, targets in data_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        loss_value = losses\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training\")\n",
    "            print(loss_dict)\n",
    "            sys.exit(1)\n",
    "\n",
    "        loss_over_epoch.append(loss_value.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "    average_loss = sum(loss_over_epoch) / len(loss_over_epoch)\n",
    "    loss_over_run.append(average_loss)\n",
    "    print(f\"EPOCH: {epoch}, LOSS: {average_loss}\")\n",
    "    lr_scheduler.step()\n",
    "\n",
    "print(\"That's it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boxes': tensor([[453.6881, 259.4587, 908.7486, 563.0987]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([0.9916], device='cuda:0', grad_fn=<IndexBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5084/45308119.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  box = draw_bounding_boxes(torch.tensor(img*255,dtype=torch.uint8), boxes=boxes,\n"
     ]
    }
   ],
   "source": [
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "model.eval()\n",
    "img, target = dataset[100]\n",
    "img_cuda = img.to(device)\n",
    "prediction = model([img_cuda])[0]\n",
    "labels = []\n",
    "boxes = []\n",
    "print(prediction)\n",
    "for i in range(len(prediction[\"labels\"])):\n",
    "    if prediction[\"scores\"][i] > 0.5:\n",
    "        labels.append(str(prediction[\"labels\"][i].item()) + \" \" + str(round(prediction[\"scores\"][i].item(), 3)))\n",
    "        boxes.append(prediction[\"boxes\"][i].cpu())\n",
    "boxes = torch.stack(boxes)\n",
    "\n",
    "box = draw_bounding_boxes(torch.tensor(img*255,dtype=torch.uint8), boxes=boxes,\n",
    "                        labels=labels,\n",
    "                        colors=\"red\",\n",
    "                        width=4) \n",
    "im = to_pil_image(box.detach())\n",
    "im.save(\"test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boxes': tensor([[ 503.,  253., 1012.,  591.],\n",
      "        [ 471.,  448.,  526.,  496.],\n",
      "        [ 510.,  433.,  562.,  454.]]), 'labels': tensor([0, 1, 1])}\n",
      "tensor([[ 476.6903,  407.3584, 1026.6876,  628.5165],\n",
      "        [ 422.3098,  263.6180, 1034.9056,  590.6732],\n",
      "        [ 659.8234,  359.4484, 1068.8989,  588.1248],\n",
      "        [ 459.3795,  355.6681,  880.9531,  578.1329],\n",
      "        [ 638.2814,  243.7959, 1131.8130,  680.9591],\n",
      "        [ 387.3619,  332.3712,  933.1053,  714.7035],\n",
      "        [ 972.7573,  495.0476, 1076.5699,  553.7318],\n",
      "        [ 489.7808,  438.8138, 1232.0000,  718.0408],\n",
      "        [ 678.5583,  275.1361, 1028.0959,  520.9781],\n",
      "        [ 493.6762,   70.4027,  971.4437,  669.9131],\n",
      "        [ 973.5790,  491.1417, 1103.7931,  586.8069],\n",
      "        [ 552.8942,  478.3871, 1085.5641,  670.7880],\n",
      "        [ 368.5137,  354.7232,  758.3600,  534.1495],\n",
      "        [ 470.1024,  321.1612,  692.7694,  609.9719],\n",
      "        [ 459.9166,  421.1677,  649.4531,  560.4008],\n",
      "        [  17.1002,  428.7850,  964.4016,  720.0000],\n",
      "        [ 608.9548,  282.3909,  871.5901,  616.3473],\n",
      "        [ 552.6012,  236.2171,  928.9848,  524.9775],\n",
      "        [ 699.2061,  315.9480,  735.9255,  354.6897],\n",
      "        [ 461.6670,  410.6393,  624.2576,  491.1868],\n",
      "        [ 792.5638,  299.2530, 1106.5203,  622.8871],\n",
      "        [ 664.5108,   52.5124, 1054.7965,  615.1992],\n",
      "        [ 151.8819,  123.7985, 1129.1875,  691.1786],\n",
      "        [ 734.8612,  444.1820, 1170.2872,  635.7631],\n",
      "        [ 623.6229,  339.5353,  727.7942,  381.7524],\n",
      "        [ 489.4950,  395.1015,  688.2211,  503.3230],\n",
      "        [ 689.7487,  285.2116,  744.8619,  364.9152],\n",
      "        [ 967.2332,  397.3462, 1092.4070,  585.7588],\n",
      "        [ 493.3406,  404.1719,  601.4282,  539.8256],\n",
      "        [ 718.2706,  287.6844,  797.5468,  357.1154],\n",
      "        [ 642.9778,  258.8757,  781.7285,  365.9066],\n",
      "        [ 371.2599,  435.1582,  775.5281,  613.7515],\n",
      "        [ 565.7156,  257.8074,  736.6464,  378.8631],\n",
      "        [ 562.9440,  394.4560,  759.5888,  510.4237],\n",
      "        [ 726.0015,  323.2038,  945.2023,  427.4824],\n",
      "        [ 523.5380,  382.1963,  645.7725,  584.0140]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "img, target = dataset[2]\n",
    "print(target)\n",
    "img = img.to(device)\n",
    "predictions = model([img])\n",
    "print(predictions[0][\"boxes\"])\n",
    "print(predictions[0][\"labels\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('eagles')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0925fae0ace981ccb72920df887c03553dc37f40775dd20c5d03ad1279f2cb4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
